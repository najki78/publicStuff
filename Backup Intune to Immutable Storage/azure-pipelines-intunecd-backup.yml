trigger: none
schedules:
  - cron: "0 1 * * 6"
    displayName: "1am every Saturday"
    branches:
      include:
        - main
    always: true
variables:
  - name: BACKUP_FOLDER
    value: prod-intune
  - name: TENANT_NAME
    value: clarios.onmicrosoft.com
  - name: USER_EMAIL
    value: IT_Intune_Backup@clarios.com
  - name: USER_NAME
    value: IT_Intune_Backup
  - name: STORAGEACCOUNT_NAME
    value: sauseast2prdeus
  - name: RESOURCEGROUP_NAME
    value: rg-useast2Prod-EUS-Automation
  - name: BACKUPCONTAINER_NAME
    value: backup-intune
  - name: SUBSCRIPTION_ID
    value: ebf92555-9049-43d9-91d9-963b798a987b
  # CLIENT_ID (Id of the SP that whould be used for authentication) and CLIENT_SECRET needs to be set separately using pipeline variables setting (so the secret is not saved as plaintext)

jobs:
  - job: backup_intune
    displayName: Backup & commit Intune configuration
    pool:
      vmImage: ubuntu-latest
    continueOnError: false
    steps:
      - checkout: self
        persistCredentials: true

      - task: Bash@3
        displayName: Remove existing prod-backup directory
        inputs:
          targetType: "inline"
          script: |
            rm -rfv "$(Build.SourcesDirectory)/$(BACKUP_FOLDER)"
          workingDirectory: "$(Build.SourcesDirectory)"
          failOnStderr: false

      - task: Bash@3
        displayName: Install IntuneCD
        inputs:
          targetType: "inline"
          script: |
            pip3 install IntuneCD
          workingDirectory: "$(Build.SourcesDirectory)"
          failOnStderr: true

      # Backup the latest configuration, using the current directory
      - task: Bash@3
        displayName: Create Intune backup
        inputs:
          targetType: "inline"
          script: |
            mkdir -p "$(Build.SourcesDirectory)/$(BACKUP_FOLDER)"

            BACKUP_START=`date +%Y.%m.%d:%H.%M.%S`
            # set BACKUP_START pipeline variable
            echo "##vso[task.setVariable variable=BACKUP_START]$BACKUP_START"

            IntuneCD-startbackup \
                --mode=1 \
                --output=json \
                --path="$(Build.SourcesDirectory)/$(BACKUP_FOLDER)" \
                --exclude CompliancePartner CompliancePartnerHeartbeat GPlaySyncTime VPPusedLicenseCount \
                --autopilot true  \
                --append-id  \
                --ignore-omasettings
          workingDirectory: "$(Build.SourcesDirectory)"
          failOnStderr: true
        env:
          TENANT_NAME: $(TENANT_NAME)
          CLIENT_ID: $(CLIENT_ID)
          CLIENT_SECRET: $(CLIENT_SECRET)

      # Commit changes and push to repo
      - task: PowerShell@2
        displayName: Find change author & commit the backup
        name: commitAndSetVariable
        inputs:
          targetType: "inline"
          script: |
            # $verbosePreference = 'continue'

            $root = "$(Build.SourcesDirectory)"

            Set-Location $root

            # configure GIT defaults
            git config --global user.name 'unknown'
            git config --global user.email 'unknown@unknown.com'
            # to avoid 256 limit on Windows
            git config --global core.longpaths true
            # to support UNICODE
            git config --global core.quotepath off
            # to avoid 'CRLF will be replaced by LF the next time Git touches it'
            git config --global core.eol lf
            git config --global core.autocrlf false

            # get changed config files
            $untrackedFile = git ls-files --others --exclude-standard --full-name
            $trackedFile = git ls-files --modified --full-name
            $changedFile = $untrackedFile, $trackedFile | % { $_ } | ? { $_ }

            # "status"
            # git --no-pager status

            # "diff"
            # git --no-pager diff

            if ($changedFile) {
                # set CHANGE_DETECTED pipeline variable
                echo "##vso[task.setVariable variable=CHANGE_DETECTED;isOutput=true;]1"

                # install required Graph modules (for authentication and getting audit logs)
                if (!(Get-Module "Microsoft.Graph.DeviceManagement.Administration" -ListAvailable)) {
                  Install-Module Microsoft.Graph.DeviceManagement.Administration -AllowClobber -Force -AcceptLicense
                }

                #region authenticate to Graph API using service principal secret
                $applicationId = "$(CLIENT_ID)"
                $securedPassword = "$(CLIENT_SECRET)"
                $tenantID = "$(TENANT_NAME)"

                $securedPasswordPassword = ConvertTo-SecureString -String $SecuredPassword -AsPlainText -Force
                $clientSecretCredential = New-Object -TypeName System.Management.Automation.PSCredential -ArgumentList $ApplicationId, $SecuredPasswordPassword
                Connect-MgGraph -TenantId $tenantID -ClientSecretCredential $ClientSecretCredential -NoWelcome
                #endregion authenticate to Graph API using service principal secret

                #region helper functions
                # function to be able to catch errors and all outputs
                function _startProcess {
                    [CmdletBinding()]
                    param (
                        [string] $filePath = ''
                        ,
                        [string] $argumentList = ''
                        ,
                        [string] $workingDirectory = (Get-Location)
                        ,
                        [switch] $dontWait
                        ,
                        # lot of git commands output verbose output to error stream
                        [switch] $outputErr2Std
                    )

                    $p = New-Object System.Diagnostics.Process
                    $p.StartInfo.UseShellExecute = $false
                    $p.StartInfo.RedirectStandardOutput = $true
                    $p.StartInfo.RedirectStandardError = $true
                    $p.StartInfo.WorkingDirectory = $workingDirectory
                    $p.StartInfo.FileName = $filePath
                    $p.StartInfo.Arguments = $argumentList
                    [void]$p.Start()
                    if (!$dontWait) {
                        $p.WaitForExit()
                    }

                    $result = $p.StandardOutput.ReadToEnd()
                    if ($result) {
                        # to avoid returning of null
                        $result
                    }
                    if ($outputErr2Std) {
                        $p.StandardError.ReadToEnd()
                    } else {
                        if ($err = $p.StandardError.ReadToEnd()) {
                            Write-Error $err
                        }
                    }
                }

                function _getResourceId {
                  [CmdletBinding()]
                  param (
                    [string] $filePath
                  )

                  $fileName = [System.IO.Path]::GetFileNameWithoutExtension($filePath)

                  # some files are just additional content for an existing config JSON and IntuneCD author decided to not put ResourceId in their name (a.k.a. resourceId needs to be retrieved from the "parent" file name)
                  # some files just don't have ResourceId in their name because of the IntunceCD author decision
                  if ($filePath -like "*Device Configurations/mobileconfig/*") {
                    $parentFolderPath = Split-Path (Split-Path $filePath -Parent) -Parent
                    $fileName = Get-ChildItem $parentFolderPath -File | ? {
                      (ConvertFrom-Json -InputObject (Get-Content $_.FullName -Raw)).payloadFileName -eq [System.IO.Path]::GetFileName($filePath)
                      } | select -expand BaseName
                    if (!$fileName) {
                      #FIXME throw az budu umet vytahnout parent file i pri DELETE operaci
                      Write-Warning "Unable to find 'parent' config file for $filePath"
                      return
                    }
                  } elseif ($filePath -like "*/Managed Google Play/*") {
                    return ($modificationEvent | ? { $_.Category -eq 'Enrollment' -and $_.ActivityType -eq "Patch AndroidForWorkSettings" }).Resources.ResourceId
                  }

                  # parse resource ID from the file name
                  # file name is in format <policyname>__<ID>
                  # beware that it doesn't have to be GUID! For example ESP profile, Apple configurator profile etc uses as ID <guid>_guid>, <guid>_string
                  $delimiter = "__"
                  if ($fileName -like "*$delimiter*") {
                    $resourceId = ($fileName -split $delimiter)[-1]
                    # just in case file name contains more than two following underscores in a row which would lead to ID starting with underscore(s)
                    $resourceId = $resourceId -replace "^_*"
                  } else {
                    $resourceId = $null
                  }

                  return $resourceId
                }
                #endregion helper functions

                # get date of the last config backup commit, to have the starting point for searching the audit log
                # because of shallow clones, I need to fetch more data before calling git log
                $gitCommitDepth = 30
                git fetch --depth=$gitCommitDepth
                $commitList = _startProcess git "--no-pager log --no-show-signature -$gitCommitDepth --format=%s%%%%%%%cI" -outputErr2Std -dontWait
                $lastCommitDate = $commitList -split "`n" | ? {$_} | % {
                  $commitName, $commitDate = $_ -split "%%%"
                  if ($commitName -match "^\d{4}\.\d{2}\.\d{2}_\d{2}\.\d{2} -- ") {
                    # config backup commit name is in a format '2023.10.08_01.01 -- ...'
                    $commitDate
                  }
                }
                if ($lastCommitDate) {
                  # pick the newest and convert it to datetime object
                  $lastCommitDate = Get-Date @($lastCommitDate)[0]
                } else {
                  Write-Warning "Unable to obtain date of the last backup config commit. ALL Intune audit events will be gathered."
                }

                # array where objects representing each changed file will be saved with information like who made the change etc
                $modificationData = New-Object System.Collections.ArrayList

                #region get all Intune audit events since the last commit
                # it is much faster to get all events at once then retrieve them one by one using resourceId
                #region create search filter
                $filter = "activityResult eq 'Success'", "ActivityOperationType ne 'Get'"

                if ($lastCommitDate) {
                  # Intune logs use UTC time
                  $lastCommitDate = $lastCommitDate.ToUniversalTime()
                  $filterDateTimeFrom = Get-Date -Date $lastCommitDate -Format "yyyy-MM-ddTHH:mm:ss"
                  $filter += "ActivityDateTime ge $filterDateTimeFrom`Z"
                }

                $backupStart = [DateTime]::ParseExact('$(BACKUP_START)', 'yyyy.MM.dd:HH.mm.ss', $null)
                $backupStart = $backupStart.ToUniversalTime()
                $filterDateTimeTo = Get-Date -Date $backupStart -Format "yyyy-MM-ddTHH:mm:ss"
                $filter += "ActivityDateTime le $filterDateTimeTo`Z"

                $eventFilter = $filter -join " and "
                #endregion create search filter

                "`nGetting Intune event logs"
                "`t- from: '$lastCommitDate' (UTC) to: '$backupStart' (UTC)"
                "`t- filter: $eventFilter"
                # Get-MgDeviceManagementAuditEvent requires DeviceManagementApps.Read.All scope
                $modificationEvent = Get-MgDeviceManagementAuditEvent -Filter $eventFilter -All
                #endregion get all Intune audit events since the last commit

                "`nProcessing changed files"
                # try to find out who made the change
                foreach ($file in $changedFile) {
                  $resourceId = _getResourceId $file

                  # get author of the resource change
                  if ($resourceId) {
                    "`t- $resourceId ($file)"

                    $resourceModificationEvent = $modificationEvent | ? { $_.Resources.ResourceId -eq $resourceId }

                    # list of change actors
                    $modificationAuthorUPN = @()

                    $resourceModificationEvent.Actor | % {
                      $actor = $_

                      if ($actor.UserPrincipalName) {
                        # modified by user
                        $modificationAuthorUPN += $actor.UserPrincipalName
                      } elseif ($actor.ApplicationDisplayName) {
                        # modified by service principal
                        $modificationAuthorUPN += ($actor.ApplicationDisplayName + " (SP)")
                      }
                    }

                    $modificationAuthorUPN = $modificationAuthorUPN | select -Unique | Sort-Object
                  } else {
                    if ($file -like "*/Assignment Report/report.json") {
                      # assignment report has no ID because it is generated by IntuneCD
                    } elseif ($file -like "*/Managed Google Play/*" -or $file -like "*Device Management Settings/settings.json" -or $file -like "*/Apple Push Notification/*") {
                      # IntuneCD don't gather those resources ID
                    } elseif ($file -like "*Device Configurations/mobileconfig/*") {
                      # IntuneCD gather those resources ID in their "parent" JSON, but when DELETE operation occurs, there is no "parent" to gather such data (at least easily)
                      #FIXME zrusit az budu umet tahat ID i pri DELETE operaci
                    } else {
                      throw "Unable to find resourceId in '$file' file name. Pipeline code modification needed, because some changes in IntuneCD were made probably."
                    }

                    $modificationAuthorUPN = $null
                  }

                  if ($modificationAuthorUPN) {
                    "`t`t- changed by: $($modificationAuthorUPN -join ', ')"
                  } else {
                    "`t`t- unable to find out who made the change"
                    $modificationAuthorUPN = 'unknown@unknown.com'
                  }

                  $null = $modificationData.Add(
                      [PSCustomObject]@{
                          resourceId            = $resourceId
                          file                  = Join-Path $root $file
                          modificationAuthorUPN = $modificationAuthorUPN
                      }
                  )
                }

                #region commit changes by author(s) who made them
                "`nCommit changes"
                # tip: grouping by created string, otherwise doesn't work correctly (probably because modificationAuthorUPN can contains multiple values)!
                $modificationData | Group-Object { $_.modificationAuthorUPN -join '&'} | % {
                    $modificationAuthorUPN = $_.Group.ModificationAuthorUPN | Select-Object -Unique
                    $modificationAuthorName = $modificationAuthorUPN | % { $_.split('@')[0] }
                    $modifiedFile = $_.Group.File

                    $modifiedFile | % {
                      "`t- Adding $_"
                      $gitResult = _startProcess git -ArgumentList "add `"$_`"" -dontWait -outputErr2Std
                      if ($gitResult -match "^fatal:") {
                        throw $gitResult
                      }
                    }

                    "`t- Setting commit author(s): $($modificationAuthorName -join ', ')"
                    git config user.name ($modificationAuthorName -join ', ')
                    git config user.email ($modificationAuthorUPN -join ', ')

                    # in case of any change in commit name, you have to modify retrieval of the $lastCommitDate too!!!
                    $DATEF = "$(Get-Date $backupStart -f yyyy.MM.dd_HH.mm)"
                    $commitName = "$DATEF` -- $($modificationAuthorName -join ', ')"

                    "`t- Creating commit '$commitName'"
                    $null = _startProcess git -ArgumentList "commit -m `"$commitName`"" -dontWait

                    $unpushedCommit = _startProcess git -ArgumentList "cherry -v origin/main"
                    if ([string]::IsNullOrEmpty($unpushedCommit)) {
                      # no change detected
                      # this shouldn't happen, it means that detection of the changed files isn't working correctly
                      Write-Warning "Nothing to commit?! This shouldn't happen."
                      # set CHANGE_DETECTED pipeline variable
                      echo "##vso[task.setVariable variable=CHANGE_DETECTED;isOutput=true;]0"
                    } else {
                      "`t`t- Commit was created"
                      # save commit date to pipeline variable to use it when creating TAG
                      echo "##vso[task.setVariable variable=COMMIT_DATE;isOutput=true;]$DATEF"
                      # save modification author(s) to use when creating TAG
                      echo "##vso[task.setVariable variable=MODIFICATION_AUTHOR;isOutput=true;]$(($modificationData.modificationAuthorUPN | select -Unique | Sort-Object) -join ', ')"
                    }
                }
                #endregion commit changes by author(s) who made them

                "`nPush changes to upstream"
                $result = _startProcess git -argumentList "push origin HEAD:main" -dontWait -outputErr2Std
              } else {
                "No change detected"
                # set CHANGE_DETECTED pipeline variable
                echo "##vso[task.setVariable variable=CHANGE_DETECTED;isOutput=true;]0"
              }

    # Create markdown documentation & commit
      - task: Bash@3
        displayName: Generate markdown document & commit
        inputs:
          targetType: 'inline'
          script: |
            if [ "$(commitAndsetVariable.CHANGE_DETECTED)" -eq 1 ]
            then
              INTRO="Intune backup and documentation generated at $(Build.Repository.Uri) <img align=\"right\" width=\"96\" height=\"96\" src=\"./logo.png\">"
              IntuneCD-startdocumentation \
                  --path="$(Build.SourcesDirectory)/prod-backup" \
                  --outpath="$(Build.SourcesDirectory)/prod-as-built.md" \
                  --tenantname=$TENANT_NAME \
                  --intro="$INTRO"

              # Commit changes and push to repo
              DATEF=`date +%Y.%m.%d`
              git config user.name $(USER_NAME)
              git config user.email $(USER_EMAIL)
              git add --all
              git commit -m "Intune config as-built $DATEF"
              
              # 2024-02-27 Lubos: this was recommended by Copilot after reporting the error with GIT PUSH:
              # Updates were rejected because a pushed branch tip is behind its remote counterpart. If you want to integrate the remote changes, use 'git pull' before pushing again. See the 'Note about fast-forwards' in 'git push --help' for details.
              git pull origin main 
              
              git push origin HEAD:main
            else
              echo "no configuration backup change detected in the last commit, documentation will not be created"
            fi
          workingDirectory: '$(Build.SourcesDirectory)'
          failOnStderr: false
        env:
          TENANT_NAME: $(TENANT_NAME)

  - job: tag
    displayName: Tag repo
    dependsOn: backup_intune
    condition: and(succeeded(), eq(dependencies.backup_intune.outputs['commitAndsetVariable.CHANGE_DETECTED'], 1))
    pool:
      vmImage: ubuntu-latest
    continueOnError: false
    variables:
      COMMIT_DATE: $[ dependencies.backup_intune.outputs['commitAndSetVariable.COMMIT_DATE'] ]
      MODIFICATION_AUTHOR: $[ dependencies.backup_intune.outputs['commitAndSetVariable.MODIFICATION_AUTHOR'] ]
    steps:
      - checkout: self
        persistCredentials: true

      # Set git global settings
      - task: Bash@3
        displayName: Configure Git
        inputs:
          targetType: "inline"
          script: |
            git config --global user.name $(USER_NAME)
            git config --global user.email $(USER_EMAIL)
          workingDirectory: "$(Build.SourcesDirectory)"
          failOnStderr: true

      - task: Bash@3
        displayName: Pull origin
        inputs:
          targetType: "inline"
          script: |
            git pull origin main
          workingDirectory: "$(Build.SourcesDirectory)"
          failOnStderr: false

      - task: PowerShell@2
        displayName: Git tag
        inputs:
          targetType: "inline"
          script: |
            # change in configuration backup folder detected, create TAG
            $DATEF= "$(COMMIT_DATE)"
            "Creating TAG '$DATEF'"
            git tag -a "$DATEF" -m "$DATEF -- Intune configuration snapshot (changes made by: $(MODIFICATION_AUTHOR))"
            git push origin "$DATEF" *> $null # even status information goes to stderr :(
          failOnStderr: true
          pwsh: false
          workingDirectory: "$(Build.SourcesDirectory)"
          
# Publish PDF & HTML documents as an artifacts
  - job: publish
    displayName: Publish as-built artifacts
    dependsOn: tag
    condition: and(succeeded(), eq(dependencies.backup_intune.outputs['commitAndsetVariable.CHANGE_DETECTED'], 1))
    pool:
      vmImage: ubuntu-latest
    continueOnError: false
    steps:
    - checkout: self
      persistCredentials: true

#   # Install md-to-pdf
#   # https://github.com/simonhaenisch/md-to-pdf
    - task: Bash@3
      displayName: Install md-to-pdf
      inputs:
        targetType: 'inline'
        script: |
          npm i --location=global md-to-pdf
        workingDirectory: '$(Build.SourcesDirectory)'
        failOnStderr: true

#   # Convert markdown document to HTML
    - task: Bash@3
      displayName: Convert markdown to HTML
      inputs:
        targetType: 'inline'
        script: |
          cat "$(Build.SourcesDirectory)/prod-as-built.md" | md-to-pdf --config-file "$(Build.SourcesDirectory)/md2pdf/htmlconfig.json" --as-html > "$(Build.SourcesDirectory)/prod-as-built.html"
        workingDirectory: '$(Build.SourcesDirectory)'
        failOnStderr: false

    - task: PublishBuildArtifacts@1
      inputs:
        pathToPublish: "$(Build.SourcesDirectory)/prod-as-built.html"
        artifactName: "prod-as-built.html"

#   # Convert markdown document to PDF
    - task: Bash@3
      displayName: Convert markdown to PDF
      inputs:
        targetType: 'inline'
        script: |
          cat "$(Build.SourcesDirectory)/prod-as-built.md" | md-to-pdf --config-file "$(Build.SourcesDirectory)/md2pdf/pdfconfig.json" > "$(Build.SourcesDirectory)/prod-as-built.pdf"
        workingDirectory: '$(Build.SourcesDirectory)'
        failOnStderr: false

    - task: PublishBuildArtifacts@1
      inputs:
        pathToPublish: "$(Build.SourcesDirectory)/prod-as-built.pdf"
        artifactName: "prod-as-built.pdf"

  - job: sync_to_immutable_container
    displayName: Sync backup directory to immutable storage container
    dependsOn: backup_intune
    # condition: and(succeeded(), eq(dependencies.backup_intune.outputs['commitAndsetVariable.CHANGE_DETECTED'], 1))
    pool:
      vmImage: windows-latest
    continueOnError: false
    steps:
      - checkout: self
        persistCredentials: true

      - task: AzurePowerShell@5
        inputs:
          azureSubscription: 'Clarios-GI-Production-001 - CL00266-Intune-Entra-Backup-ReadOnly'
          ScriptType: 'InlineScript'
          Inline: |
            try {
              $null = Set-AzContext -SubscriptionId $(SUBSCRIPTION_ID) 
              $ctx = New-AzStorageContext -StorageAccountName $(STORAGEACCOUNT_NAME) -UseConnectedAccount  
              
              $root = "$(Build.SourcesDirectory)"
              $DefWorkingDir = "$(System.DefaultWorkingDirectory)"
              
              Set-Location $DefWorkingDir 
              
              $null = md "$DefWorkingDir\azcopy"
              Invoke-WebRequest -Uri "https://aka.ms/downloadazcopy-v10-windows" -OutFile "AzCopy.zip"
              
              # Load the assembly required for zip file manipulation
              Add-Type -AssemblyName System.IO.Compression.FileSystem
              
              # Open the zip archive
              $zipArchive = [System.IO.Compression.ZipFile]::OpenRead("$DefWorkingDir\azcopy.zip")
              
              # Iterate through each entry in the zip file
              foreach ($entry in $zipArchive.Entries) {
                  # Define the destination path for the current entry
                  # This example extracts all files directly into the destination directory without preserving the folder structure
                  #$destinationPath = Join-Path -Path $destinationDirectory -ChildPath $entry.Name
                  #$entry
              
                  # Check if the entry is a directory (we skip directories)
                  if (-not $entry.FullName.EndsWith("/")) {
                      # Extract the file to the destination path
                      [System.IO.Compression.ZipFileExtensions]::ExtractToFile($entry, "$DefWorkingDir\AzCopy\$($entry.Name)", $true)
                  }
              }
              # Close the zip archive
              $zipArchive.Dispose()
              Remove-Item "AzCopy.zip" -Force
              
              # instruct AZCOPY how to authenticate
              $Env:AZCOPY_AUTO_LOGIN_TYPE="PSCRED"
              
              Write-Host "Source: $($root)\$(BACKUP_FOLDER)"
              
              # AZCOPY SYNC does not currently work properly with an immutable storage - it does not add new versions of the changed files 
              # but it is very useful for the 1st ever copy to a new immutable storage (for fast full copy)
              # & "$DefWorkingDir\AzCopy\azcopy.exe" sync "$($root)\$(BACKUP_FOLDER)" "https://$(STORAGEACCOUNT_NAME).blob.core.windows.net/$(BACKUPCONTAINER_NAME)" --compare-hash MD5 --put-md5 --log-level info --recursive # --delete-destination true # --dry-run # --output-type json

              # List all blobs in a container
              $blobList = (Get-AzStorageBlob -Container $(BACKUPCONTAINER_NAME) -Context $ctx).Name
              
              $localFolder = "$($root)\$(BACKUP_FOLDER)"

              $uploadCounter = 0
              
              $allLocalFiles = Get-ChildItem -Path $localFolder -Recurse -File
              
              foreach ( $localFile in $allLocalFiles ) {
                $localFilePath = $localFile.FullName
              
                $localRelativePath = ($localFile.FullName).Replace($localFolder + "\","") # removing leading folder name
                $localRelativePath = $localRelativePath.Replace("\","/")
              
                $uploadFlag = $false # $true indicates that the local file should be copied to the storage container
              
                $blobName = $blobList | Where-Object { $_ -eq $localRelativePath }             
                
                if ( $blobName ) {
                  # If blob exists, compare the MD5 hash of the local file with the blob's Content-MD5 property
              
                  # https://github.com/Azure/azure-powershell/issues/10049
                  # Get-AzStorageBlob doesn't return blobs with [ or ] in name, use this workaround:
                  $modifiedBlobName = $blobName -replace '(\[|\])', '[$1]'
                  
                  # get MD5
              
                    try {
                      # Get the blob
                      $blob = Get-AzStorageBlob -Container $(BACKUPCONTAINER_NAME) -Blob $modifiedBlobName -Context $ctx
                      
                    } catch {
                      Write-Host "Exception: Unable to retrieve the blob: $blobName ($modifiedBlobName)"
                    }
              
                    try {
                      $hexString = $null
                      $byteArray = $null
              
                      # Fetch the attributes of the blob - SUPER IMPORTANT, OTHERWISE MD5 PROPERTY IS NOT AVAILABLE
                      $blob.ICloudBlob.FetchAttributes()
              
                      $byteArray = [System.Convert]::FromBase64String($blob.ICloudBlob.Properties.ContentMD5)
                      $hexString = [System.BitConverter]::ToString($byteArray).Replace("-", "").ToLower()
              
                    } catch {
                      Write-Host "Exception: Unable to retrieve the MD5 of the blob: $blobName ($hexString)"
                    }
              
                  # if MD5 not stored with the file, download the blob and calculate it
                  if(-not $hexString) {
              
                    try {
                      # Download the blob
                      $blobContent = Get-AzStorageBlobContent -Container $(BACKUPCONTAINER_NAME) -Blob $modifiedBlobName -Context $ctx
              
                      # Calculate the MD5 hash - alternative approach
                      #$md5 = New-Object -TypeName System.Security.Cryptography.MD5CryptoServiceProvider
                      #$hash = [System.BitConverter]::ToString($md5.ComputeHash([System.IO.File]::ReadAllBytes($blobContent.Name)))
                      #Write-Host "[System.BitConverter]: $($hash)"
                    
                      # Compute the hash of a file
                      $hash = Get-FileHash -LiteralPath $blobContent.Name -Algorithm MD5
                      $hexString = $hash.hash.ToLower()
                      Write-Host "MD5 not stored with the file. Get-FileHash: $hexString"
              
                    } catch {
                      Write-Host "Unable to calculate the hash of blob: $blobName ($hexString)"
                      $hexString = $null
                    }
                    
                  }
              
                  # if hash was not with the file, nor calculated...
                  if(-not $hexString) {
                    $uploadFlag = $true
                  } else {
              
                    try {
                      # check the local hash - always use -Literalpath to prevent errors (I learnt it a hard way)
                      $localFileHash = Get-FileHash -LiteralPath $localFilePath -Algorithm MD5
                      if($localFileHash) {
                        #Write-Host "localFileHash.hash.ToLower: $($localFileHash.hash.ToLower())"
                      } else {
                        Write-Host "Hash of the local file was not calculated (investigate why not!): $localFilePath"
                        $uploadFlag = $true
                      }
                    } catch {
                      Write-Host "Exception: Unable to calculate the hash of local file: $localFilePath"
                      $uploadFlag = $true
                    }                  
              
                  }
              
                  # are the two hashes iqual?
                  if($hexString -and $localFileHash -and ($localFileHash.hash.ToLower() -eq $hexString)) {
                    #Write-Host "Local and blob files are the same"
                  } else {
                    Write-Host "Local and blob files are different. Local file to be copied and added as a new version of the blob.: $localRelativePath"
                    $uploadFlag = $true
                  }
              
                } else {
                    Write-Host "The file does not exist in the blob list, copying it: $localRelativePath"
                    $uploadFlag = $true
                }
              
                # shall we copy the file?
                if ($uploadFlag) {
                  Write-Host "Copying: $localRelativePath"
                  & "$DefWorkingDir\AzCopy\azcopy.exe" copy "$($root)\$(BACKUP_FOLDER)\$localRelativePath" "https://$(STORAGEACCOUNT_NAME).blob.core.windows.net/$(BACKUPCONTAINER_NAME)/$localRelativePath" --put-md5 --log-level info --recursive # --dry-run # --output-type json
                  $uploadCounter += 1
                } else {
                  #Write-Host "Skipping $blobName ($uploadFlag)"
                }
              
              }
              
              #############################################################
              # and delete those files that are no longer in the source...
              #############################################################
              
              # to be implemented ... 
              
              Write-Host "DONE: Sync backup - $uploadCounter file(s) copied"
            } catch {
              $_  # This will display the error message
              Write-Host "Exception."
            }
          azurePowerShellVersion: 'LatestVersion'
          pwsh: true